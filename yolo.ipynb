{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VeDQvhYZ0HPe",
        "colab_type": "code",
        "outputId": "31b5fcd8-3028-430d-f5a6-efa45205c1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rizkidewanto/pisang"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pisang'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 164 (delta 53), reused 151 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (164/164), 30.47 MiB | 14.11 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sfmNZePlS9bc",
        "colab_type": "code",
        "outputId": "04152a14-8013-408a-bd9e-f79af5a9b045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "%cd pisang/yolo\n",
        "#!python3 setup.py build_ext --inplace\n",
        "!pip install . #Install Keseluruhan\n",
        "!wget https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar #Nantinya Download dataset pisang\n",
        "!tar xf VOCtrainval_11-May-2012.tar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pisang/yolo\n",
            "Processing /content/pisang/yolo\n",
            "Building wheels for collected packages: darkflow\n",
            "  Building wheel for darkflow (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-kkso_cqy/wheels/6a/bd/a3/403ed4ca3a9a98be8ef1b4a79867d1a085f2b237811da57c06\n",
            "Successfully built darkflow\n",
            "Installing collected packages: darkflow\n",
            "Successfully installed darkflow-1.0.0\n",
            "--2019-04-07 00:05:31--  https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/octet-stream]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  22.3MB/s    in 87s     \n",
            "\n",
            "2019-04-07 00:06:59 (22.0 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n2ee3LO3qOTK",
        "colab_type": "code",
        "outputId": "5135162e-3b3c-4a83-e5c3-dd65dfedc22e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5674
        }
      },
      "cell_type": "code",
      "source": [
        "#Train menggunakan CPU\n",
        "!flow --model cfg/yolo.cfg --train --dataset \"VOCdevkit/VOC2012/JPEGImages\" --annotation \"VOCdevkit/VOC2012/Annotations\"\n",
        "\n",
        "#Train menggunakan GPU\n",
        "#!flow --model cfg/yolo.cfg --train --dataset \"VOCdevkit/VOC2012/JPEGImages\" --annotation \"VOCdevkit/VOC2012/Annotations\" --gpu 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Parsing cfg/yolo.cfg\n",
            "Loading None ...\n",
            "Finished in 0.00016069412231445312s\n",
            "Model has a coco model name, loading coco labels.\n",
            "\n",
            "Building net ...\n",
            "Source | Train? | Layer description                | Output size\n",
            "-------+--------+----------------------------------+---------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "       |        | input                            | (?, 608, 608, 3)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
            " Init  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
            " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
            " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
            "-------+--------+----------------------------------+---------------\n",
            "Running entirely on CPU\n",
            "cfg/yolo.cfg loss hyper-parameters:\n",
            "\tH       = 19\n",
            "\tW       = 19\n",
            "\tbox     = 5\n",
            "\tclasses = 80\n",
            "\tscales  = [1.0, 5.0, 1.0, 1.0]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/darkflow/net/yolov2/train.py:87: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Building cfg/yolo.cfg loss\n",
            "Building cfg/yolo.cfg train op\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019-04-07 02:41:32.628058: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-07 02:41:32.628565: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3309700 executing computations on platform Host. Devices:\n",
            "2019-04-07 02:41:32.628609: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-07 02:41:32.728500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-07 02:41:32.729056: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x33092e0 executing computations on platform CUDA. Devices:\n",
            "2019-04-07 02:41:32.729093: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-04-07 02:41:32.729239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-07 02:41:32.729264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      \n",
            "Finished in 12.335018634796143s\n",
            "\n",
            "Enter training ...\n",
            "\n",
            "cfg/yolo.cfg parsing VOCdevkit/VOC2012/Annotations\n",
            "Parsing for ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] \n",
            "[====================>]100%  2008_005875.xml\n",
            "Statistics:\n",
            "bird: 1271\n",
            "dog: 1598\n",
            "train: 704\n",
            "pottedplant: 1202\n",
            "bus: 685\n",
            "sofa: 841\n",
            "person: 17401\n",
            "cow: 771\n",
            "tvmonitor: 893\n",
            "motorbike: 801\n",
            "car: 2492\n",
            "horse: 803\n",
            "chair: 3056\n",
            "cat: 1277\n",
            "bicycle: 837\n",
            "sheep: 1084\n",
            "aeroplane: 1002\n",
            "bottle: 1561\n",
            "diningtable: 800\n",
            "boat: 1059\n",
            "Dataset size: 17125\n",
            "Dataset of 17125 instance(s)\n",
            "Training statistics: \n",
            "\tLearning rate : 1e-05\n",
            "\tBatch size    : 16\n",
            "\tEpoch number  : 1000\n",
            "\tBackup every  : 2000\n",
            "tcmalloc: large alloc 1703567360 bytes == 0x7f05b7e16000 @  0x7f06afa841e7 0x7f06937bec85 0x7f06938356de 0x7f0693ac8688 0x7f0693ae911a 0x7f0693ae9579 0x7f0693aea8e1 0x7f068c74db7d 0x7f068c740f95 0x7f068c7c5ef7 0x7f068c7c2b28 0x7f06ae36657f 0x7f06af4486db 0x7f06af78188f\n",
            "step 1 - loss 228.36293029785156 - moving ave loss 228.3629302978516\n",
            "tcmalloc: large alloc 1703567360 bytes == 0x6fb74000 @  0x7f06afa841e7 0x7f06937bec85 0x7f06938356de 0x7f0693ac8688 0x7f0693ae911a 0x7f0693ae9579 0x7f0693aea8e1 0x7f068c74db7d 0x7f068c740f95 0x7f068c7c5ef7 0x7f068c7c2b28 0x7f06ae36657f 0x7f06af4486db 0x7f06af78188f\n",
            "step 2 - loss 228.2957000732422 - moving ave loss 228.35620727539066\n",
            "tcmalloc: large alloc 1703567360 bytes == 0x9cd74000 @  0x7f06afa841e7 0x7f06937bec85 0x7f06938356de 0x7f0693ac8688 0x7f0693ae911a 0x7f0693ae9579 0x7f0693aea8e1 0x7f068c74db7d 0x7f068c740f95 0x7f068c7c5ef7 0x7f068c7c2b28 0x7f06ae36657f 0x7f06af4486db 0x7f06af78188f\n",
            "step 3 - loss 227.70584106445312 - moving ave loss 228.2911706542969\n",
            "tcmalloc: large alloc 1703567360 bytes == 0x7f04eaf34000 @  0x7f06afa841e7 0x7f06937bec85 0x7f06938356de 0x7f0693ac8688 0x7f0693ae911a 0x7f0693ae9579 0x7f0693aea8e1 0x7f068c74db7d 0x7f068c740f95 0x7f068c7c5ef7 0x7f068c7c2b28 0x7f06ae36657f 0x7f06af4486db 0x7f06af78188f\n",
            "step 4 - loss 228.38661193847656 - moving ave loss 228.30071478271486\n",
            "step 5 - loss 229.501708984375 - moving ave loss 228.42081420288088\n",
            "step 6 - loss 229.10276794433594 - moving ave loss 228.4890095770264\n",
            "step 7 - loss 228.47967529296875 - moving ave loss 228.48807614862065\n",
            "step 8 - loss 228.4646453857422 - moving ave loss 228.4857330723328\n",
            "step 9 - loss 228.91552734375 - moving ave loss 228.52871249947455\n",
            "step 10 - loss 228.72915649414062 - moving ave loss 228.54875689894118\n",
            "step 11 - loss 228.48748779296875 - moving ave loss 228.54262998834395\n",
            "step 12 - loss 228.31175231933594 - moving ave loss 228.51954222144315\n",
            "step 13 - loss 227.63577270507812 - moving ave loss 228.43116526980666\n",
            "step 14 - loss 227.85142517089844 - moving ave loss 228.37319125991584\n",
            "step 15 - loss 228.23582458496094 - moving ave loss 228.35945459242038\n",
            "step 16 - loss 228.516845703125 - moving ave loss 228.37519370349085\n",
            "step 17 - loss 227.26332092285156 - moving ave loss 228.26400642542694\n",
            "step 18 - loss 228.436767578125 - moving ave loss 228.28128254069674\n",
            "step 19 - loss 228.21665954589844 - moving ave loss 228.2748202412169\n",
            "step 20 - loss 228.25990295410156 - moving ave loss 228.27332851250537\n",
            "step 21 - loss 227.58358764648438 - moving ave loss 228.2043544259033\n",
            "step 22 - loss 227.54220581054688 - moving ave loss 228.13813956436763\n",
            "step 23 - loss 228.3214111328125 - moving ave loss 228.15646672121215\n",
            "step 24 - loss 227.57550048828125 - moving ave loss 228.0983700979191\n",
            "step 25 - loss 227.29852294921875 - moving ave loss 228.01838538304906\n",
            "step 26 - loss 227.9630126953125 - moving ave loss 228.0128481142754\n",
            "step 27 - loss 228.68666076660156 - moving ave loss 228.08022937950804\n",
            "step 28 - loss 227.32809448242188 - moving ave loss 228.00501588979944\n",
            "step 29 - loss 227.58456420898438 - moving ave loss 227.96297072171797\n",
            "step 30 - loss 226.89056396484375 - moving ave loss 227.85573004603054\n",
            "step 31 - loss 227.6802978515625 - moving ave loss 227.83818682658375\n",
            "step 32 - loss 228.64968872070312 - moving ave loss 227.9193370159957\n",
            "step 33 - loss 228.50938415527344 - moving ave loss 227.97834172992347\n",
            "step 34 - loss 227.61819458007812 - moving ave loss 227.94232701493894\n",
            "step 35 - loss 229.71090698242188 - moving ave loss 228.11918501168725\n",
            "step 36 - loss 228.3839111328125 - moving ave loss 228.1456576237998\n",
            "step 37 - loss 226.82516479492188 - moving ave loss 228.01360834091201\n",
            "step 38 - loss 227.181884765625 - moving ave loss 227.93043598338335\n",
            "step 39 - loss 228.61044311523438 - moving ave loss 227.99843669656846\n",
            "step 40 - loss 226.87631225585938 - moving ave loss 227.88622425249756\n",
            "step 41 - loss 227.69003295898438 - moving ave loss 227.86660512314626\n",
            "step 42 - loss 227.45960998535156 - moving ave loss 227.8259056093668\n",
            "step 43 - loss 227.50672912597656 - moving ave loss 227.79398796102777\n",
            "step 44 - loss 227.75794982910156 - moving ave loss 227.79038414783514\n",
            "step 45 - loss 228.89697265625 - moving ave loss 227.90104299867664\n",
            "step 46 - loss 227.30972290039062 - moving ave loss 227.84191098884804\n",
            "step 47 - loss 227.64439392089844 - moving ave loss 227.8221592820531\n",
            "step 48 - loss 227.16796875 - moving ave loss 227.75674022884778\n",
            "step 49 - loss 227.95169067382812 - moving ave loss 227.77623527334583\n",
            "step 50 - loss 227.59930419921875 - moving ave loss 227.75854216593314\n",
            "step 51 - loss 228.3341827392578 - moving ave loss 227.81610622326562\n",
            "step 52 - loss 227.68026733398438 - moving ave loss 227.8025223343375\n",
            "step 53 - loss 226.42730712890625 - moving ave loss 227.6650008137944\n",
            "step 54 - loss 227.940185546875 - moving ave loss 227.69251928710247\n",
            "step 55 - loss 228.87203979492188 - moving ave loss 227.81047133788442\n",
            "step 56 - loss 226.88307189941406 - moving ave loss 227.7177313940374\n",
            "step 57 - loss 226.96212768554688 - moving ave loss 227.64217102318835\n",
            "step 58 - loss 226.5797576904297 - moving ave loss 227.53592968991248\n",
            "step 59 - loss 227.62530517578125 - moving ave loss 227.54486723849936\n",
            "step 60 - loss 228.89434814453125 - moving ave loss 227.67981532910255\n",
            "step 61 - loss 226.3798828125 - moving ave loss 227.5498220774423\n",
            "step 62 - loss 228.21633911132812 - moving ave loss 227.61647378083086\n",
            "step 63 - loss 228.01254272460938 - moving ave loss 227.65608067520873\n",
            "step 64 - loss 226.58584594726562 - moving ave loss 227.54905720241442\n",
            "step 65 - loss 227.50155639648438 - moving ave loss 227.54430712182142\n",
            "step 66 - loss 227.61798095703125 - moving ave loss 227.55167450534242\n",
            "step 67 - loss 227.43881225585938 - moving ave loss 227.54038828039413\n",
            "step 68 - loss 227.5755615234375 - moving ave loss 227.54390560469847\n",
            "step 69 - loss 227.16741943359375 - moving ave loss 227.50625698758802\n",
            "step 70 - loss 226.79067993164062 - moving ave loss 227.4346992819933\n",
            "step 71 - loss 227.59521484375 - moving ave loss 227.45075083816897\n",
            "step 72 - loss 227.35711669921875 - moving ave loss 227.44138742427396\n",
            "step 73 - loss 226.45770263671875 - moving ave loss 227.34301894551845\n",
            "step 74 - loss 226.22021484375 - moving ave loss 227.23073853534163\n",
            "step 75 - loss 227.5711669921875 - moving ave loss 227.26478138102624\n",
            "step 76 - loss 226.66958618164062 - moving ave loss 227.2052618610877\n",
            "step 77 - loss 225.96861267089844 - moving ave loss 227.08159694206878\n",
            "step 78 - loss 227.04339599609375 - moving ave loss 227.07777684747128\n",
            "step 79 - loss 226.889404296875 - moving ave loss 227.05893959241166\n",
            "step 80 - loss 226.47903442382812 - moving ave loss 227.0009490755533\n",
            "step 81 - loss 226.56298828125 - moving ave loss 226.957152996123\n",
            "step 82 - loss 226.42019653320312 - moving ave loss 226.90345734983103\n",
            "step 83 - loss 227.69192504882812 - moving ave loss 226.98230411973074\n",
            "step 84 - loss 226.25027465820312 - moving ave loss 226.909101173578\n",
            "step 85 - loss 226.36575317382812 - moving ave loss 226.854766373603\n",
            "step 86 - loss 225.99986267089844 - moving ave loss 226.76927600333255\n",
            "step 87 - loss 226.55184936523438 - moving ave loss 226.74753333952276\n",
            "step 88 - loss 226.48927307128906 - moving ave loss 226.72170731269938\n",
            "step 89 - loss 226.94662475585938 - moving ave loss 226.74419905701538\n",
            "step 90 - loss 226.46446228027344 - moving ave loss 226.71622537934118\n",
            "step 91 - loss 225.681884765625 - moving ave loss 226.61279131796957\n",
            "step 92 - loss 225.88478088378906 - moving ave loss 226.5399902745515\n",
            "step 93 - loss 225.9886474609375 - moving ave loss 226.48485599319014\n",
            "step 94 - loss 227.1236572265625 - moving ave loss 226.54873611652738\n",
            "step 95 - loss 225.8323516845703 - moving ave loss 226.47709767333168\n",
            "step 96 - loss 226.1583251953125 - moving ave loss 226.44522042552978\n",
            "step 97 - loss 226.11721801757812 - moving ave loss 226.41242018473463\n",
            "step 98 - loss 227.25668334960938 - moving ave loss 226.4968465012221\n",
            "step 99 - loss 226.0977783203125 - moving ave loss 226.45693968313117\n",
            "step 100 - loss 226.32308959960938 - moving ave loss 226.443554674779\n",
            "step 101 - loss 227.38246154785156 - moving ave loss 226.53744536208626\n",
            "step 102 - loss 227.35824584960938 - moving ave loss 226.61952541083858\n",
            "step 103 - loss 225.9420166015625 - moving ave loss 226.55177452991097\n",
            "step 104 - loss 225.3123016357422 - moving ave loss 226.42782724049408\n",
            "step 105 - loss 226.15554809570312 - moving ave loss 226.400599326015\n",
            "step 106 - loss 225.6956329345703 - moving ave loss 226.33010268687053\n",
            "step 107 - loss 225.82012939453125 - moving ave loss 226.2791053576366\n",
            "step 108 - loss 226.54354858398438 - moving ave loss 226.3055496802714\n",
            "step 109 - loss 226.65113830566406 - moving ave loss 226.3401085428107\n",
            "step 110 - loss 226.3234100341797 - moving ave loss 226.3384386919476\n",
            "step 111 - loss 225.37550354003906 - moving ave loss 226.24214517675676\n",
            "step 112 - loss 226.58677673339844 - moving ave loss 226.27660833242092\n",
            "step 113 - loss 225.30355834960938 - moving ave loss 226.17930333413977\n",
            "step 114 - loss 226.36915588378906 - moving ave loss 226.19828858910472\n",
            "step 115 - loss 225.43704223632812 - moving ave loss 226.12216395382706\n",
            "step 116 - loss 225.47006225585938 - moving ave loss 226.0569537840303\n",
            "step 117 - loss 225.9635467529297 - moving ave loss 226.04761308092026\n",
            "step 118 - loss 225.44879150390625 - moving ave loss 225.98773092321886\n",
            "step 119 - loss 225.58377075195312 - moving ave loss 225.9473349060923\n",
            "step 120 - loss 225.61492919921875 - moving ave loss 225.91409433540494\n",
            "step 121 - loss 225.78225708007812 - moving ave loss 225.90091060987226\n",
            "step 122 - loss 225.43763732910156 - moving ave loss 225.8545832817952\n",
            "step 123 - loss 225.67295837402344 - moving ave loss 225.836420791018\n",
            "step 124 - loss 224.83148193359375 - moving ave loss 225.7359269052756\n",
            "step 125 - loss 226.31597900390625 - moving ave loss 225.79393211513866\n",
            "Checkpoint at step 125\n",
            "step 126 - loss 225.1707763671875 - moving ave loss 225.73161654034357\n",
            "step 127 - loss 226.90293884277344 - moving ave loss 225.84874877058655\n",
            "step 128 - loss 225.94525146484375 - moving ave loss 225.8583990400123\n",
            "step 129 - loss 224.67410278320312 - moving ave loss 225.73996941433137\n",
            "step 130 - loss 226.06686401367188 - moving ave loss 225.77265887426543\n",
            "step 131 - loss 225.06304931640625 - moving ave loss 225.7016979184795\n",
            "step 132 - loss 224.89736938476562 - moving ave loss 225.62126506510813\n",
            "step 133 - loss 224.9573516845703 - moving ave loss 225.55487372705434\n",
            "step 134 - loss 225.01206970214844 - moving ave loss 225.50059332456374\n",
            "step 135 - loss 225.3167266845703 - moving ave loss 225.48220666056443\n",
            "step 136 - loss 225.57736206054688 - moving ave loss 225.4917222005627\n",
            "step 137 - loss 224.8354034423828 - moving ave loss 225.4260903247447\n",
            "step 138 - loss 225.15380859375 - moving ave loss 225.39886215164523\n",
            "step 139 - loss 226.14109802246094 - moving ave loss 225.4730857387268\n",
            "step 140 - loss 225.5187530517578 - moving ave loss 225.4776524700299\n",
            "step 141 - loss 224.83169555664062 - moving ave loss 225.413056778691\n",
            "step 142 - loss 226.10647583007812 - moving ave loss 225.4823986838297\n",
            "step 143 - loss 226.13601684570312 - moving ave loss 225.54776050001703\n",
            "step 144 - loss 224.987548828125 - moving ave loss 225.49173933282785\n",
            "step 145 - loss 225.4024658203125 - moving ave loss 225.4828119815763\n",
            "step 146 - loss 224.37596130371094 - moving ave loss 225.37212691378977\n",
            "step 147 - loss 225.04661560058594 - moving ave loss 225.33957578246938\n",
            "step 148 - loss 225.47216796875 - moving ave loss 225.35283500109745\n",
            "step 149 - loss 225.611328125 - moving ave loss 225.37868431348772\n",
            "step 150 - loss 224.8572540283203 - moving ave loss 225.326541284971\n",
            "step 151 - loss 224.44700622558594 - moving ave loss 225.2385877790325\n",
            "step 152 - loss 225.0317840576172 - moving ave loss 225.217907406891\n",
            "step 153 - loss 225.84429931640625 - moving ave loss 225.28054659784254\n",
            "step 154 - loss 224.69802856445312 - moving ave loss 225.2222947945036\n",
            "step 155 - loss 224.76132202148438 - moving ave loss 225.1761975172017\n",
            "step 156 - loss 225.20758056640625 - moving ave loss 225.17933582212217\n",
            "step 157 - loss 224.30950927734375 - moving ave loss 225.09235316764435\n",
            "step 158 - loss 225.07351684570312 - moving ave loss 225.09046953545024\n",
            "step 159 - loss 224.51393127441406 - moving ave loss 225.03281570934664\n",
            "step 160 - loss 224.81280517578125 - moving ave loss 225.0108146559901\n",
            "step 161 - loss 224.19723510742188 - moving ave loss 224.92945670113326\n",
            "step 162 - loss 224.37969970703125 - moving ave loss 224.87448100172307\n",
            "step 163 - loss 224.38453674316406 - moving ave loss 224.82548657586716\n",
            "step 164 - loss 225.14198303222656 - moving ave loss 224.8571362215031\n",
            "step 165 - loss 224.24061584472656 - moving ave loss 224.79548418382547\n",
            "step 166 - loss 225.45849609375 - moving ave loss 224.86178537481794\n",
            "step 167 - loss 224.14208984375 - moving ave loss 224.78981582171116\n",
            "step 168 - loss 227.41514587402344 - moving ave loss 225.0523488269424\n",
            "step 169 - loss 224.91558837890625 - moving ave loss 225.0386727821388\n",
            "step 170 - loss 224.28475952148438 - moving ave loss 224.96328145607336\n",
            "step 171 - loss 224.97802734375 - moving ave loss 224.96475604484104\n",
            "step 172 - loss 225.53497314453125 - moving ave loss 225.02177775481007\n",
            "step 173 - loss 224.00347900390625 - moving ave loss 224.9199478797197\n",
            "step 174 - loss 224.12010192871094 - moving ave loss 224.83996328461882\n",
            "step 175 - loss 224.31625366210938 - moving ave loss 224.78759232236789\n",
            "step 176 - loss 225.10130310058594 - moving ave loss 224.8189634001897\n",
            "step 177 - loss 226.02462768554688 - moving ave loss 224.9395298287254\n",
            "step 178 - loss 223.52175903320312 - moving ave loss 224.79775274917318\n",
            "step 179 - loss 224.04461669921875 - moving ave loss 224.72243914417777\n",
            "step 180 - loss 223.09982299804688 - moving ave loss 224.5601775295647\n",
            "step 181 - loss 223.6100311279297 - moving ave loss 224.4651628894012\n",
            "step 182 - loss 223.4649200439453 - moving ave loss 224.36513860485564\n",
            "step 183 - loss 225.53248596191406 - moving ave loss 224.4818733405615\n",
            "step 184 - loss 224.04908752441406 - moving ave loss 224.43859475894675\n",
            "step 185 - loss 223.693603515625 - moving ave loss 224.36409563461459\n",
            "step 186 - loss 224.09307861328125 - moving ave loss 224.33699393248125\n",
            "step 187 - loss 223.77017211914062 - moving ave loss 224.28031175114717\n",
            "step 188 - loss 225.06643676757812 - moving ave loss 224.35892425279027\n",
            "step 189 - loss 223.9124755859375 - moving ave loss 224.314279386105\n",
            "step 190 - loss 224.06509399414062 - moving ave loss 224.28936084690858\n",
            "step 191 - loss 223.56045532226562 - moving ave loss 224.2164702944443\n",
            "step 192 - loss 224.1953125 - moving ave loss 224.21435451499988\n",
            "step 193 - loss 223.23492431640625 - moving ave loss 224.11641149514054\n",
            "step 194 - loss 224.02098083496094 - moving ave loss 224.10686842912258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mb-VNq5I9_js",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}